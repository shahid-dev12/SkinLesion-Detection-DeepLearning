{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4193778,"sourceType":"datasetVersion","datasetId":2473232}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom collections import Counter\n\n# Image Processing\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\n\n# Model Building\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# Data Balancing\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Visualization Style\nsns.set_style('darkgrid')\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load metadata\ndata = pd.read_csv('/kaggle/input/isic-2019-skin-lesion-images-for-classification/ISIC_2019_Training_GroundTruth.csv')\nprint(\"Data Shape:\", data.shape)\n\n# Check image folder structure\ndataset_path = '/kaggle/input/isic-2019-skin-lesion-images-for-classification/'\nprint(os.listdir(dataset_path))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_resize_images(image_paths, target_size=(32, 32)):\n    images = []\n    for path in image_paths:\n        img = load_img(path, target_size=target_size)  # Load and resize\n        img_array = img_to_array(img)  # Convert to array\n        images.append(img_array)\n    return np.array(images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define classes\nclass_folders = ['MEL', 'VASC', 'SCC', 'DF', 'NV', 'BKL', 'BCC', 'AK']\n\n# Load images and labels\nall_images, all_labels = [], []\n\nfor class_name in class_folders:\n    class_folder_path = os.path.join(dataset_path, class_name)\n    image_files = os.listdir(class_folder_path)\n    class_images = load_and_resize_images([os.path.join(class_folder_path, img) for img in image_files])\n    all_images.append(class_images)\n    all_labels.append([class_name] * len(image_files))\n\n# Combine all data\nX_all = np.concatenate(all_images, axis=0)\ny_all = np.concatenate(all_labels, axis=0)\n\nprint(\"Shape of all images:\", X_all.shape)\nprint(\"Shape of all labels:\", y_all.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_all)\ny_one_hot = to_categorical(y_encoded)\n\n# Reshape data for oversampling\nX_reshaped = X_all.reshape(X_all.shape[0], -1)\noversampler = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = oversampler.fit_resample(X_reshaped, y_one_hot)\n\nprint(\"Class distribution after oversampling:\", Counter(np.argmax(y_resampled, axis=1)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\nprint(f\"Shapes - X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshape to (28, 28, 3)\nX_train_reshaped = X_train.reshape(X_train.shape[0], 32, 32, 3)\nX_test_reshaped = X_test.reshape(X_test.shape[0], 32, 32, 3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50, InceptionV3\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ncallbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n]\n# Define input shape\ninput_shape = (32, 32, 3)\nnum_classes = 8  # Adjust based on your dataset\n\n# Load Pretrained Models\n\ndef build_resnet50():\n    base_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n# Train and Evaluate Models\ndef train_and_evaluate(model, name):\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n                        epochs=10, batch_size=64, callbacks=callbacks)\n    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n    print(f\"{name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n    return history, test_accuracy\n\n# Train models\nmodels = {\n    \"ResNet50\": build_resnet50(),\n}\n\nhistories, accuracies = {}, {}\nfor name, model in models.items():\n    histories[name], accuracies[name] = train_and_evaluate(model, name)\n\n# Plot Comparison\nplt.figure(figsize=(10, 5))\nplt.bar(accuracies.keys(), accuracies.values(), color=['red', 'blue', 'green', 'purple'])\nplt.xlabel(\"Model\")\nplt.ylabel(\"Test Accuracy\")\nplt.title(\"Comparison of CNN Models\")\nplt.show()\n","metadata":{"trusted":true,"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Define callback\ncallbacks = [EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)]\n\n# Define input shape & number of classes\ninput_shape = (32, 32, 3)\nnum_classes = 8  # Adjust based on your dataset\n\n# Build ResNet50 Model\ndef build_resnet50():\n    base_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n# Train & Save Model\ndef train_and_save(model, model_name=\"ResNet50\"):\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n                        epochs=10, batch_size=64, callbacks=callbacks)\n    \n    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n    print(f\"{model_name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n    # Save model\n    model.save(f\"{model_name}.h5\")\n\n    # Save training history and accuracy in JSON\n    history_dict = history.history\n    history_dict[\"test_accuracy\"] = test_accuracy  # Store test accuracy separately\n\n    with open(f\"{model_name}_history.json\", \"w\") as f:\n        json.dump(history_dict, f)\n\n    return history, test_accuracy\n\n# Train and Save ResNet50\nmodel = build_resnet50()\nhistory, accuracy = train_and_save(model)\n\n# Save Accuracy Separately\naccuracy_data = {\"ResNet50\": accuracy}\nwith open(\"model_accuracies.json\", \"w\") as f:\n    json.dump(accuracy_data, f)\n\n# Plot & Save Accuracy Graph\nplt.figure(figsize=(6, 4))\nplt.bar([\"ResNet50\"], [accuracy * 100], color=\"blue\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Test Accuracy (%)\")\nplt.title(\"ResNet50 Accuracy\")\nplt.savefig(\"resnet50_accuracy.png\")  # Save plot as an image file\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"inceptionNET-----------------------","metadata":{}},{"cell_type":"code","source":"def load_and_resize_images(image_paths, target_size=(75, 75)):\n    images = []\n    for path in image_paths:\n        img = load_img(path, target_size=target_size)  # Load and resize\n        img_array = img_to_array(img)  # Convert to array\n        images.append(img_array)\n    return np.array(images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom collections import Counter\n\n# Image Processing\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\n\n# Model Building\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# Define classes\nclass_folders = ['MEL', 'VASC', 'SCC', 'DF', 'NV', 'BKL', 'BCC', 'AK']\n\n# Load images and labels\nall_images, all_labels = [], []\n\nfor class_name in class_folders:\n    class_folder_path = os.path.join(dataset_path, class_name)\n    image_files = os.listdir(class_folder_path)\n    class_images = load_and_resize_images([os.path.join(class_folder_path, img) for img in image_files])\n    all_images.append(class_images)\n    all_labels.append([class_name] * len(image_files))\n\n# Combine all data\nX_all = np.concatenate(all_images, axis=0)\ny_all = np.concatenate(all_labels, axis=0)\n\nprint(\"Shape of all images:\", X_all.shape)\nprint(\"Shape of all labels:\", y_all.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_all)\ny_one_hot = to_categorical(y_encoded)\n\n# Reshape data for oversampling\nX_reshaped = X_all.reshape(X_all.shape[0], -1)\noversampler = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = oversampler.fit_resample(X_reshaped, y_one_hot)\n\nprint(\"Class distribution after oversampling:\", Counter(np.argmax(y_resampled, axis=1)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\nprint(f\"Shapes - X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshape to (28, 28, 3)\nX_train_reshaped = X_train.reshape(X_train.shape[0], 75, 75, 3)\nX_test_reshaped = X_test.reshape(X_test.shape[0], 75, 75, 3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50, InceptionV3\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ncallbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n]\n# Define input shape\ninput_shape = (75, 75, 3)\nnum_classes = 8  # Adjust based on your dataset\n\n# Load Pretrained Models\ndef build_inception_v1():\n    base_model = InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n\n\n\n\n\n# Train and Evaluate Models\ndef train_and_evaluate(model, name):\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n                        epochs=5, batch_size=64, callbacks=callbacks)\n    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n    print(f\"{name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n    return history, test_accuracy\n\n# Train models\nmodels = {\n    \"InceptionV1\": build_inception_v1(),\n}\n\nhistories, accuracies = {}, {}\nfor name, model in models.items():\n    histories[name], accuracies[name] = train_and_evaluate(model, name)\n\n# Plot Comparison\nplt.figure(figsize=(10, 5))\nplt.bar(accuracies.keys(), accuracies.values(), color=['red', 'blue', 'green', 'purple'])\nplt.xlabel(\"Model\")\nplt.ylabel(\"Test Accuracy\")\nplt.title(\"Comparison of CNN Models\")\nplt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Define callback\ncallbacks = [EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)]\n\n# Define input shape & number of classes\ninput_shape = (75, 75, 3)\nnum_classes = 8  # Adjust based on your dataset\n\n# Build InceptionV3 Model\ndef build_inception_v3():\n    base_model = InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n# Train & Save Model\ndef train_and_save(model, model_name=\"InceptionV3\"):\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n                        epochs=10, batch_size=64, callbacks=callbacks)\n    \n    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n    print(f\"{model_name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n    # Save model\n    model.save(f\"{model_name}.h5\")\n\n    # Save training history and accuracy in JSON\n    history_dict = history.history\n    history_dict[\"test_accuracy\"] = test_accuracy  # Store test accuracy separately\n\n    with open(f\"{model_name}_history.json\", \"w\") as f:\n        json.dump(history_dict, f)\n\n    return history, test_accuracy\n\n# Train and Save InceptionV3\nmodel = build_inception_v3()\nhistory, accuracy = train_and_save(model, \"InceptionV3\")\n\n# Save Accuracy Separately\naccuracy_data = {\"InceptionV3\": accuracy}\nwith open(\"model_accuracies.json\", \"w\") as f:\n    json.dump(accuracy_data, f)\n\n# Plot & Save Accuracy Graph\nplt.figure(figsize=(6, 4))\nplt.bar([\"InceptionV3\"], [accuracy * 100], color=\"green\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Test Accuracy (%)\")\nplt.title(\"InceptionV3 Accuracy\")\nplt.savefig(\"inceptionv3_accuracy.png\")  # Save plot as an image file\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ALEXNET_______________________","metadata":{}},{"cell_type":"code","source":"def load_and_resize_images(image_paths, target_size=(128, 128)):  # Resize correctly for AlexNet\n    images = []\n    for path in image_paths:\n        img = load_img(path, target_size=target_size)  # Resize the image\n        img_array = img_to_array(img)  # Convert to array\n        images.append(img_array)\n    return np.array(images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define classes\nclass_folders = ['MEL', 'VASC', 'SCC', 'DF', 'NV', 'BKL', 'BCC', 'AK']\n# Load images and labels\nall_images, all_labels = [], []\n\nfor class_name in class_folders:\n    class_folder_path = os.path.join(dataset_path, class_name)\n    image_files = os.listdir(class_folder_path)\n    class_images = load_and_resize_images([os.path.join(class_folder_path, img) for img in image_files], target_size=(128, 128))\n    all_images.append(class_images)\n    all_labels.append([class_name] * len(image_files))\n\n# Combine all data\nX_all = np.concatenate(all_images, axis=0)\ny_all = np.concatenate(all_labels, axis=0)\n\nprint(\"Shape of all images:\", X_all.shape)  # Should be (total_images, 227, 227, 3)\nprint(\"Shape of all labels:\", y_all.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now the images are correctly sized, and we can proceed with splitting\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n\n# No need to reshape manually, as images are already (227, 227, 3)\nprint(f\"Shapes - X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\ndef build_alexnet(input_shape=(128, 128, 3), num_classes=8):  \n    model = Sequential([\n        Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape),  # Modify kernel size if needed\n        MaxPooling2D((3, 3), strides=(2, 2)),\n        \n        Conv2D(256, (5, 5), padding='same', activation='relu'),\n        MaxPooling2D((3, 3), strides=(2, 2)),\n        \n        Conv2D(384, (3, 3), padding='same', activation='relu'),\n        Conv2D(384, (3, 3), padding='same', activation='relu'),\n        Conv2D(256, (3, 3), padding='same', activation='relu'),\n        MaxPooling2D((3, 3), strides=(2, 2)),\n        \n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the model\nalexnet_model = build_alexnet()\nalexnet_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert class labels (strings) to numeric values\nlabel_encoder = LabelEncoder()\ny_all_numeric = label_encoder.fit_transform(y_all)  # Convert string labels to numbers\n\n# Split Data (Ensure you already have X_train, X_test)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all_numeric, test_size=0.2, random_state=42)\n\n# One-hot encode the labels\ny_train = to_categorical(y_train, num_classes=8)\ny_test = to_categorical(y_test, num_classes=8)\n\n# Verify shapes\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define callback\ncallbacks = [EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)]\n# Reshape to (28, 28, 3)\nX_train_reshaped = X_train.reshape(X_train.shape[0], 128, 128, 3)\nX_test_reshaped = X_test.reshape(X_test.shape[0], 128, 128, 3)# Train & Save Model\n\ndef train_and_save(model, model_name=\"AlexNet\"):\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    history = model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test),\n                        epochs=10, batch_size=64, callbacks=callbacks)\n    \n    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n    print(f\"{model_name} Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n    # Save model\n    model.save(f\"{model_name}.h5\")\n\n    # Save training history and accuracy in JSON\n    history_dict = history.history\n    history_dict[\"test_accuracy\"] = test_accuracy  # Store test accuracy separately\n\n    with open(f\"{model_name}_history.json\", \"w\") as f:\n        json.dump(history_dict, f)\n\n    return history, test_accuracy\n\n# Train and Save AlexNet\nalexnet_model = build_alexnet()\nhistory, accuracy = train_and_save(alexnet_model, \"AlexNet\")\n\n# Save Accuracy Separately\naccuracy_data = {\"AlexNet\": accuracy}\nwith open(\"model_accuracies.json\", \"w\") as f:\n    json.dump(accuracy_data, f)\n\n# Plot & Save Accuracy Graph\nplt.figure(figsize=(6, 4))\nplt.bar([\"AlexNet\"], [accuracy * 100], color=\"blue\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Test Accuracy (%)\")\nplt.title(\"AlexNet Accuracy\")\nplt.savefig(\"alexnet_accuracy.png\")  # Save plot as an image file\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"novality-----CNN deep architecture ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Activation, \n                                     MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, Add, Concatenate)\n\ndef residual_block(x, filters):\n    shortcut = x  # Save input for residual connection\n    \n    x = Conv2D(filters, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    x = Conv2D(filters, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    \n    x = Add()([x, shortcut])  # Residual connection\n    x = Activation('relu')(x)\n    return x\n\ndef dense_block(x, filters):\n    conv1 = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n    conv2 = Conv2D(filters, (3, 3), padding='same', activation='relu')(conv1)\n    \n    x = Concatenate()([x, conv2])  # Dense connection\n    return x\n\ndef build_novel_cnn(input_shape=(75, 75, 3), num_classes=8):\n    inputs = Input(shape=input_shape)\n    \n    # Initial Convolution\n    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(inputs)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n    \n    # Residual & Dense Blocks\n    x = residual_block(x, 64)\n    x = dense_block(x, 64)\n    x = MaxPooling2D((2, 2))(x)\n    \n    x = residual_block(x, 128)\n    x = dense_block(x, 128)\n    x = MaxPooling2D((2, 2))(x)\n    \n    x = residual_block(x, 256)\n    x = dense_block(x, 256)\n    x = MaxPooling2D((2, 2))(x)\n    \n    # Global Pooling & Fully Connected Layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs, x, name=\"Novel_CNN\")\n    return model\n\n# Create the model\nnovel_cnn = build_novel_cnn()\nnovel_cnn.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
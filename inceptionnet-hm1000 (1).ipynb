{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\n# Bibliothèques Python\nimport os, cv2,itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\n\n# Bibliothèques PyTorch\nimport torch\nfrom torch import optim,nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import models,transforms\n\n# Bibliothèques scikit-learn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Assurer la reproductibilité des résultats\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\n\n# Vérification du contenu du répertoire\nprint(os.listdir('/kaggle/input/'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:45:39.232289Z","iopub.execute_input":"2025-05-12T13:45:39.232620Z","iopub.status.idle":"2025-05-12T13:45:44.502062Z","shell.execute_reply.started":"2025-05-12T13:45:39.232587Z","shell.execute_reply":"2025-05-12T13:45:44.501303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vérification du contenu du répertoire d'images\nprint(os.listdir('../input/skin-cancer-mnist-ham10000'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:45:48.846907Z","iopub.execute_input":"2025-05-12T13:45:48.847331Z","iopub.status.idle":"2025-05-12T13:45:48.853444Z","shell.execute_reply.started":"2025-05-12T13:45:48.847307Z","shell.execute_reply":"2025-05-12T13:45:48.852579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Définir le répertoire des données\ndata_dir = '../input/skin-cancer-mnist-ham10000'\n\n# Récupérer les chemins des images (fichiers .jpg)\nall_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n\n# Créer un dictionnaire pour associer l'ID de l'image au chemin\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n\n# Dictionnaire des types de lésions cutanées\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:45:55.120019Z","iopub.execute_input":"2025-05-12T13:45:55.120292Z","iopub.status.idle":"2025-05-12T13:45:55.376561Z","shell.execute_reply.started":"2025-05-12T13:45:55.120272Z","shell.execute_reply":"2025-05-12T13:45:55.375771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom tqdm import tqdm\n\ndef compute_img_mean_std(image_paths):\n    \"\"\"\n    Computing the mean and std of three channels for the whole dataset,\n    first we should normalize the image from 0-255 to 0-1.\n    \n    Note: This is adjusted for InceptionV3 which requires 299x299 input size.\n    \"\"\"\n    \n    # Set the target size to 299x299 for InceptionV3\n    img_h, img_w = 299, 299\n    imgs = []\n    means, stdevs = [], []\n\n    # Loop through each image in the dataset\n    for i in tqdm(range(len(image_paths))):\n        # Read the image\n        img = cv2.imread(image_paths[i])\n        \n        # Resize the image to 299x299\n        img = cv2.resize(img, (img_w, img_h))\n        \n        # Append the resized image to the list\n        imgs.append(img)\n\n    # Stack images into a numpy array (shape: [height, width, channels, num_images])\n    imgs = np.stack(imgs, axis=3)  # [H, W, C, N]\n    print(imgs.shape)  # [299, 299, 3, N]\n\n    # Convert images to float32 and normalize to [0, 1]\n    imgs = imgs.astype(np.float32) / 255.0\n\n    # Compute the mean and std for each channel (RGB)\n    for i in range(3):  # Loop through the 3 channels (RGB)\n        # Flatten the pixel values of the i-th channel\n        pixels = imgs[:, :, i, :].ravel()  # Flatten to a 1D array\n        means.append(np.mean(pixels))  # Calculate mean\n        stdevs.append(np.std(pixels))  # Calculate standard deviation\n\n    # Reverse the lists to match RGB order (OpenCV loads images in BGR)\n    means.reverse()  \n    stdevs.reverse()\n\n    # Print the calculated mean and standard deviation values\n    print(\"normMean = {}\".format(means))\n    print(\"normStd = {}\".format(stdevs))\n\n    # Return the mean and standard deviation\n    return means, stdevs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:06.501152Z","iopub.execute_input":"2025-05-12T13:46:06.501494Z","iopub.status.idle":"2025-05-12T13:46:06.508355Z","shell.execute_reply.started":"2025-05-12T13:46:06.501468Z","shell.execute_reply":"2025-05-12T13:46:06.507399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Valeurs moyennes et écart-types normalisés pour les canaux R, G, B\nnorm_mean = [0.7630392, 0.5456477, 0.57004845]\nnorm_std = [0.1409286, 0.15261266, 0.16997074]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:11.351431Z","iopub.execute_input":"2025-05-12T13:46:11.351706Z","iopub.status.idle":"2025-05-12T13:46:11.355330Z","shell.execute_reply.started":"2025-05-12T13:46:11.351686Z","shell.execute_reply":"2025-05-12T13:46:11.354501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chargement des métadonnées\ndf_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n\n# Ajout du chemin d'accès à chaque image\ndf_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n\n# Ajout du type de lésion cutanée à partir du dictionnaire\ndf_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n\n# Conversion du type de lésion en indices numériques\ndf_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n\ndf_original.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:14.041030Z","iopub.execute_input":"2025-05-12T13:46:14.041416Z","iopub.status.idle":"2025-05-12T13:46:14.108139Z","shell.execute_reply.started":"2025-05-12T13:46:14.041381Z","shell.execute_reply":"2025-05-12T13:46:14.107256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Regroupement par 'lesion_id' et comptage des occurrences\ndf_undup = df_original.groupby('lesion_id').count()\n\n# Filtrer les 'lesion_id' qui ont seulement une image associée\ndf_undup = df_undup[df_undup['image_id'] == 1]\n\n# Réinitialisation de l'index pour simplifier l'accès\ndf_undup.reset_index(inplace=True)\n\ndf_undup.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:17.381422Z","iopub.execute_input":"2025-05-12T13:46:17.381719Z","iopub.status.idle":"2025-05-12T13:46:17.410448Z","shell.execute_reply.started":"2025-05-12T13:46:17.381697Z","shell.execute_reply":"2025-05-12T13:46:17.409748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fonction pour identifier les doublons dans les lésions\ndef get_duplicates(x):\n    unique_list = list(df_undup['lesion_id'])\n    if x in unique_list:\n        return 'unduplicated'\n    else:\n        return 'duplicated'\n\n# Création d'une nouvelle colonne qui est une copie de la colonne 'lesion_id'\ndf_original['duplicates'] = df_original['lesion_id']\n\n# Application de la fonction pour marquer les doublons\ndf_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n\ndf_original.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:20.378585Z","iopub.execute_input":"2025-05-12T13:46:20.378995Z","iopub.status.idle":"2025-05-12T13:46:24.404858Z","shell.execute_reply.started":"2025-05-12T13:46:20.378962Z","shell.execute_reply":"2025-05-12T13:46:24.404010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Comptage des valeurs dans la colonne 'duplicates' pour savoir combien de doublons et non-doublons il y a\ndf_original['duplicates'].value_counts().reset_index(name='count')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:27.464034Z","iopub.execute_input":"2025-05-12T13:46:27.464377Z","iopub.status.idle":"2025-05-12T13:46:27.475982Z","shell.execute_reply.started":"2025-05-12T13:46:27.464329Z","shell.execute_reply":"2025-05-12T13:46:27.475129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filtrer les lésions qui sont marquées comme 'unduplicated'\ndf_undup = df_original[df_original['duplicates'] == 'unduplicated']\ndf_undup.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:30.803039Z","iopub.execute_input":"2025-05-12T13:46:30.803368Z","iopub.status.idle":"2025-05-12T13:46:30.812020Z","shell.execute_reply.started":"2025-05-12T13:46:30.803322Z","shell.execute_reply":"2025-05-12T13:46:30.811098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df_undup['cell_type_idx']\n\n# Création d'un ensemble de validation en utilisant train_test_split\n_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n\n# Affichage de la forme de l'ensemble de validation pour vérifier sa taille\ndf_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:33.814947Z","iopub.execute_input":"2025-05-12T13:46:33.815415Z","iopub.status.idle":"2025-05-12T13:46:33.831314Z","shell.execute_reply.started":"2025-05-12T13:46:33.815371Z","shell.execute_reply":"2025-05-12T13:46:33.830152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vérification de la répartition des classes dans l'ensemble de validation\ndf_val['cell_type_idx'].value_counts().reset_index(name='count')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:38.398100Z","iopub.execute_input":"2025-05-12T13:46:38.398430Z","iopub.status.idle":"2025-05-12T13:46:38.409086Z","shell.execute_reply.started":"2025-05-12T13:46:38.398401Z","shell.execute_reply":"2025-05-12T13:46:38.408051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# Identify train and val rows\n# Create a new colum that is a copy of the image_id column\ndf_original['train_or_val'] = df_original['image_id']\n# Apply the function to this new column\ndf_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n# Filter out training rows\ndf_train = df_original[df_original['train_or_val'] == 'train']\nprint(len(df_train))\nprint(len(df_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:46:41.605225Z","iopub.execute_input":"2025-05-12T13:46:41.605561Z","iopub.status.idle":"2025-05-12T13:46:42.464014Z","shell.execute_reply.started":"2025-05-12T13:46:41.605534Z","shell.execute_reply":"2025-05-12T13:46:42.463294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['cell_type_idx'].value_counts().reset_index(name='count')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:18.935646Z","iopub.execute_input":"2025-05-12T13:47:18.935926Z","iopub.status.idle":"2025-05-12T13:47:18.944797Z","shell.execute_reply.started":"2025-05-12T13:47:18.935903Z","shell.execute_reply":"2025-05-12T13:47:18.943763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_val['cell_type'].value_counts().reset_index(name='count')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:26.728472Z","iopub.execute_input":"2025-05-12T13:47:26.728740Z","iopub.status.idle":"2025-05-12T13:47:26.737255Z","shell.execute_reply.started":"2025-05-12T13:47:26.728720Z","shell.execute_reply":"2025-05-12T13:47:26.736545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Vérifiez si CUDA (GPU) est disponible\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA is available: {cuda_available}\")\n\n# Si le GPU est disponible, utilisez-le, sinon utilisez le CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Affichez le nom du GPU, si disponible\nif cuda_available:\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"No GPU found.\")\n\n# Exemple de tensor déplacé vers le GPU\ntensor = torch.randn(3, 3).to(device)\nprint(f\"Tensor on device: {tensor.device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:34.326275Z","iopub.execute_input":"2025-05-12T13:47:34.326567Z","iopub.status.idle":"2025-05-12T13:47:34.637957Z","shell.execute_reply.started":"2025-05-12T13:47:34.326547Z","shell.execute_reply":"2025-05-12T13:47:34.637248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Initialiser les taux d'augmentation souhaités\ndata_aug_rate = [15, 10, 5, 50, 0, 40, 5]\n\n# Calculer le nombre maximum d'exemples parmi toutes les classes (pour le but d'équilibrage)\nmax_size = df_train['cell_type'].value_counts().max()\n\n# Créer un DataFrame pour enregistrer les nouvelles données augmentées\naugmented_data = []\n\n# Boucle sur chaque classe et augmenter de manière équilibrée\nfor i in range(7):\n    # Récupérer le sous-ensemble de données pour chaque classe\n    class_data = df_train[df_train['cell_type_idx'] == i]\n    \n    # Calculer le nombre d'exemples à augmenter pour atteindre la taille cible\n    target_size = max_size  # ou une taille cible définie manuellement\n    current_size = len(class_data)\n    augmentation_factor = (target_size // current_size)  # combien de fois multiplier les données de la classe\n    \n    # Si augmentation nécessaire\n    if augmentation_factor > 1:\n        augmented_class_data = pd.concat([class_data] * augmentation_factor, ignore_index=True)\n        augmented_data.append(augmented_class_data)\n\n# Concaténer toutes les classes augmentées avec les données d'origine\ndf_train_augmented = pd.concat([df_train] + augmented_data, ignore_index=True)\n\n# Vérifier les nouvelles valeurs de 'cell_type' après augmentation\nprint(df_train_augmented['cell_type'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:40.308873Z","iopub.execute_input":"2025-05-12T13:47:40.309145Z","iopub.status.idle":"2025-05-12T13:47:40.345180Z","shell.execute_reply.started":"2025-05-12T13:47:40.309125Z","shell.execute_reply":"2025-05-12T13:47:40.344519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the test set again in a validation set and a true test set:\ndf_val, df_test = train_test_split(df_val, test_size=0.5)\ndf_train = df_train.reset_index()\ndf_val = df_val.reset_index()\ndf_test = df_test.reset_index()\n\nprint(len(df_test))\ndf_test['cell_type'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:44.567378Z","iopub.execute_input":"2025-05-12T13:47:44.567691Z","iopub.status.idle":"2025-05-12T13:47:44.584153Z","shell.execute_reply.started":"2025-05-12T13:47:44.567667Z","shell.execute_reply":"2025-05-12T13:47:44.583218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:48.327820Z","iopub.execute_input":"2025-05-12T13:47:48.328100Z","iopub.status.idle":"2025-05-12T13:47:48.332224Z","shell.execute_reply.started":"2025-05-12T13:47:48.328079Z","shell.execute_reply":"2025-05-12T13:47:48.331422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\n# Fonction pour initialiser le modèle\ndef initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    model_ft = None\n    input_size = 0\n\n    # Vérifier si le modèle demandé est 'inception'\n    if model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        # Charger le modèle InceptionV3 avec des poids pré-entraînés (ImageNet par défaut)\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n\n        # Fonction pour geler ou ajuster les paramètres du modèle selon l'option 'feature_extract'\n        set_parameter_requires_grad(model_ft, feature_extract)\n\n        # Traiter le réseau auxiliaire (AuxLogits)\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n\n        # Traiter le réseau principal (fc)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n\n        # Définir la taille d'entrée attendue par le modèle\n        input_size = 299  # Taille d'entrée pour InceptionV3\n\n    else:\n        # Si le modèle spécifié n'est pas supporté, afficher un message d'erreur et arrêter l'exécution\n        print(\"Nom du modèle invalide, sortie...\")\n        exit()\n\n    # Retourner le modèle initialisé et la taille d'entrée\n    return model_ft, input_size\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:51.012799Z","iopub.execute_input":"2025-05-12T13:47:51.013168Z","iopub.status.idle":"2025-05-12T13:47:51.020249Z","shell.execute_reply.started":"2025-05-12T13:47:51.013138Z","shell.execute_reply":"2025-05-12T13:47:51.019166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Paramètres du modèle\nmodel_name = \"inception\"  # Choix du modèle (ici InceptionV3)\nnum_classes = 7  # Nombre de classes pour la classification\nfeature_extract = False  # Indique si on veut uniquement extraire des caractéristiques (True) ou entraîner entièrement (False)\n\n# Initialisation du modèle avec les paramètres définis ci-dessus\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Définir l'appareil (GPU ou CPU) sur lequel le modèle sera exécuté\ndevice = torch.device('cuda:0')  # Utilisation du GPU (cuda:0 pour le premier GPU disponible)\n# Si vous souhaitez utiliser le CPU au lieu du GPU, vous pouvez décommenter la ligne suivante :\n# device = torch.device('cpu')  # Utilisation du CPU\n\n# Déplacer le modèle sur l'appareil sélectionné (GPU ou CPU)\nmodel = model_ft.to(device)\n\n# Vérification que le modèle est bien sur l'appareil choisi\nprint(f\"Le modèle est placé sur : {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:47:54.846203Z","iopub.execute_input":"2025-05-12T13:47:54.846526Z","iopub.status.idle":"2025-05-12T13:47:55.930411Z","shell.execute_reply.started":"2025-05-12T13:47:54.846500Z","shell.execute_reply":"2025-05-12T13:47:55.929443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Définir les transformations pour les images d'entraînement\ntrain_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),\n                                      transforms.RandomRotation(20),\n                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n                                      transforms.ToTensor(), \n                                      transforms.Normalize(norm_mean, norm_std)])\n\n# Définir les transformations pour les images de validation\nval_transform = transforms.Compose([transforms.Resize((input_size,input_size)), \n                                    transforms.ToTensor(),\n                                    transforms.Normalize(norm_mean, norm_std)])\n\n# Définir les transformations pour les images de test\ntest_transform = transforms.Compose([transforms.Resize((input_size,input_size)), \n                                     transforms.ToTensor(),\n                                    transforms.Normalize(norm_mean, norm_std)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:00.580138Z","iopub.execute_input":"2025-05-12T13:48:00.580470Z","iopub.status.idle":"2025-05-12T13:48:00.585888Z","shell.execute_reply.started":"2025-05-12T13:48:00.580442Z","shell.execute_reply":"2025-05-12T13:48:00.585043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HAM10000(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        # Load data and get label\n        X = Image.open(self.df['path'][index])\n        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n\n        if self.transform:\n            X = self.transform(X)\n\n        return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:03.658109Z","iopub.execute_input":"2025-05-12T13:48:03.658426Z","iopub.status.idle":"2025-05-12T13:48:03.663161Z","shell.execute_reply.started":"2025-05-12T13:48:03.658398Z","shell.execute_reply":"2025-05-12T13:48:03.662261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Définir l'ensemble d'entraînement en utilisant le DataFrame train_df et les transformations définies (train_transform)\ntraining_set = HAM10000(df_train, transform=train_transform)\ntrain_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n\n# Définir de la même manière l'ensemble de validation :\nvalidation_set = HAM10000(df_val, transform=train_transform)\nval_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)\n\n# Définir de la même manière l'ensemble de test :\ntest_set = HAM10000(df_test, transform=train_transform)\ntest_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:07.339209Z","iopub.execute_input":"2025-05-12T13:48:07.339532Z","iopub.status.idle":"2025-05-12T13:48:07.344388Z","shell.execute_reply.started":"2025-05-12T13:48:07.339505Z","shell.execute_reply":"2025-05-12T13:48:07.343707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:10.763204Z","iopub.execute_input":"2025-05-12T13:48:10.763521Z","iopub.status.idle":"2025-05-12T13:48:10.768058Z","shell.execute_reply.started":"2025-05-12T13:48:10.763494Z","shell.execute_reply":"2025-05-12T13:48:10.767050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom typing import Optional, List\nfrom torch.nn.modules.loss import _Loss\n\n# Constants for mode\nBINARY_MODE = \"binary\"\nMULTICLASS_MODE = \"multiclass\"\nMULTILABEL_MODE = \"multilabel\"\n\n# Dice Loss class\nclass DiceLoss(_Loss):\n    def __init__(\n        self,\n        mode: str,\n        classes: Optional[List[int]] = None,\n        log_loss: bool = False,\n        from_logits: bool = True,\n        smooth: float = 0.0,\n        ignore_index: Optional[int] = None,\n        eps: float = 1e-7,\n    ):\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DiceLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            self.classes = torch.tensor(classes, dtype=torch.long)\n        else:\n            self.classes = None\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.log_loss = log_loss\n        self.ignore_index = ignore_index\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask.unsqueeze(1)\n                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)\n                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)\n            else:\n                y_true = F.one_hot(y_true, num_classes)\n                y_true = y_true.permute(0, 2, 1)\n\n        scores = self.compute_score(\n            y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims\n        )\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        mask = y_true.sum(dims) > 0\n        loss *= mask.to(loss.dtype)\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n\n    def compute_score(\n        self, output, target, smooth=0.0, eps=1e-7, dims=None\n    ) -> torch.Tensor:\n        intersection = torch.sum(output * target, dims)\n        cardinality = torch.sum(output + target, dims)\n        return (2. * intersection + smooth) / (cardinality + smooth + eps)\n\n\n# Créez votre optimiseur et perte\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Assurez-vous que votre modèle Inception est sur le bon appareil\n\n# Fonction de perte\ncriterion = DiceLoss(mode=MULTICLASS_MODE, smooth=1e-6).to(device)\n\n# Optimiseur Adam\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nfrom torch.optim.lr_scheduler import StepLR\n\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Réduit le LR toutes les 5 époques\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:13.507965Z","iopub.execute_input":"2025-05-12T13:48:13.508232Z","iopub.status.idle":"2025-05-12T13:48:13.521790Z","shell.execute_reply.started":"2025-05-12T13:48:13.508211Z","shell.execute_reply":"2025-05-12T13:48:13.521022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_loss_train, total_acc_train = [],[]\ndef train(train_loader, model, criterion, optimizer, epoch):\n    model.train()\n    train_loss = AverageMeter()\n    train_acc = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        images, labels = data\n        N = images.size(0)\n        images = Variable(images).to(device)\n        labels = Variable(labels).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        outputs = outputs[0]\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        prediction = outputs.max(1, keepdim=True)[1]\n        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n        train_loss.update(loss.item())\n        curr_iter += 1\n        if (i + 1) % 100 == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n            total_loss_train.append(train_loss.avg)\n            total_acc_train.append(train_acc.avg)\n    return train_loss.avg, train_acc.avg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:21.419613Z","iopub.execute_input":"2025-05-12T13:48:21.419902Z","iopub.status.idle":"2025-05-12T13:48:21.426215Z","shell.execute_reply.started":"2025-05-12T13:48:21.419879Z","shell.execute_reply":"2025-05-12T13:48:21.425377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(val_loader, model, criterion, optimizer, epoch):\n    model.eval()\n    val_loss = AverageMeter()\n    val_acc = AverageMeter()\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(val_loader)):\n            images, labels = data\n            N = images.size(0)\n            images = Variable(images).to(device)\n            labels = Variable(labels).to(device)\n\n            outputs = model(images)\n            prediction = outputs.max(1, keepdim=True)[1]\n\n            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n\n            val_loss.update(criterion(outputs, labels).item())\n\n    print('------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n    print('------------------------------------------------------------')\n    return val_loss.avg, val_acc.avg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:26.015544Z","iopub.execute_input":"2025-05-12T13:48:26.015849Z","iopub.status.idle":"2025-05-12T13:48:26.021119Z","shell.execute_reply.started":"2025-05-12T13:48:26.015825Z","shell.execute_reply":"2025-05-12T13:48:26.020363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epoch_num = 10\nbest_val_acc = 0\ntotal_loss_val, total_acc_val = [], []\n\nfor epoch in tqdm(range(1, epoch_num + 1)):\n    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n    total_loss_val.append(loss_val)\n    total_acc_val.append(acc_val)\n    scheduler.step(loss_val) \n    if acc_val > best_val_acc:\n        best_val_acc = acc_val\n        print('*****************************************************')\n        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n        print('*****************************************************')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T13:48:30.607173Z","iopub.execute_input":"2025-05-12T13:48:30.607493Z","iopub.status.idle":"2025-05-12T14:11:59.160884Z","shell.execute_reply.started":"2025-05-12T13:48:30.607467Z","shell.execute_reply":"2025-05-12T14:11:59.159882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(num = 1)\nfig2 = fig.add_subplot(1,1,1)\nfig2.plot(total_acc_val, label = 'validation accuracy')\nfig2.plot(total_loss_val, label = 'validation loss')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:12:25.977670Z","iopub.execute_input":"2025-05-12T14:12:25.977953Z","iopub.status.idle":"2025-05-12T14:12:26.222863Z","shell.execute_reply.started":"2025-05-12T14:12:25.977930Z","shell.execute_reply":"2025-05-12T14:12:26.222013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(num=1)\nfig1 = fig.add_subplot(1,1,1)\nfig1.plot(total_acc_train, label = 'training accuracy')\nfig1.plot(total_loss_train, label = 'training loss')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:12:30.270472Z","iopub.execute_input":"2025-05-12T14:12:30.270769Z","iopub.status.idle":"2025-05-12T14:12:30.500828Z","shell.execute_reply.started":"2025-05-12T14:12:30.270747Z","shell.execute_reply":"2025-05-12T14:12:30.500152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:12:42.637592Z","iopub.execute_input":"2025-05-12T14:12:42.637917Z","iopub.status.idle":"2025-05-12T14:12:42.643946Z","shell.execute_reply.started":"2025-05-12T14:12:42.637890Z","shell.execute_reply":"2025-05-12T14:12:42.642845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validation data evaluation\n\nmodel.eval()\ny_label = []\ny_predict = []\nwith torch.no_grad():\n    for i, data in enumerate(val_loader):\n        images, labels = data\n        N = images.size(0)\n        images = Variable(images).to(device)\n        outputs = model(images)\n        prediction = outputs.max(1, keepdim=True)[1]\n        y_label.extend(labels.cpu().numpy())\n        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_label, y_predict)\n# plot the confusion matrix\nplot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\nplot_confusion_matrix(confusion_mtx, plot_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:12:46.698570Z","iopub.execute_input":"2025-05-12T14:12:46.698849Z","iopub.status.idle":"2025-05-12T14:12:52.419619Z","shell.execute_reply.started":"2025-05-12T14:12:46.698829Z","shell.execute_reply":"2025-05-12T14:12:52.418586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test data evaluation\n\nmodel.eval()\ntest_y_label = []\ntest_y_predict = []\nwith torch.no_grad():\n    for i, data in enumerate(test_loader):\n        images, labels = data\n        N = images.size(0)\n        images = Variable(images).to(device)\n        outputs = model(images)\n        prediction = outputs.max(1, keepdim=True)[1]\n        test_y_label.extend(labels.cpu().numpy())\n        test_y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n\n# compute the confusion matrix\nconfusion_mtx_test = confusion_matrix(test_y_label, test_y_predict)\n# plot the confusion matrix\nplot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\nplot_confusion_matrix(confusion_mtx, plot_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:12:58.182857Z","iopub.execute_input":"2025-05-12T14:12:58.183181Z","iopub.status.idle":"2025-05-12T14:13:04.610461Z","shell.execute_reply.started":"2025-05-12T14:12:58.183156Z","shell.execute_reply":"2025-05-12T14:13:04.609521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate a validation classification report\nreport = classification_report(y_label, y_predict, target_names=plot_labels)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:13:08.178952Z","iopub.execute_input":"2025-05-12T14:13:08.179294Z","iopub.status.idle":"2025-05-12T14:13:08.195188Z","shell.execute_reply.started":"2025-05-12T14:13:08.179267Z","shell.execute_reply":"2025-05-12T14:13:08.194191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate a test classification report\nreport = classification_report(test_y_label, test_y_predict, target_names=plot_labels)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:13:12.265878Z","iopub.execute_input":"2025-05-12T14:13:12.266151Z","iopub.status.idle":"2025-05-12T14:13:12.279065Z","shell.execute_reply.started":"2025-05-12T14:13:12.266130Z","shell.execute_reply":"2025-05-12T14:13:12.278371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\nplt.bar(np.arange(7),label_frac_error)\nplt.xlabel('True Label')\nplt.ylabel('Fraction classified incorrectly')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:13:21.550183Z","iopub.execute_input":"2025-05-12T14:13:21.550514Z","iopub.status.idle":"2025-05-12T14:13:21.814103Z","shell.execute_reply.started":"2025-05-12T14:13:21.550485Z","shell.execute_reply":"2025-05-12T14:13:21.813244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_frac_error = 1 - np.diag(confusion_mtx_test) / np.sum(confusion_mtx_test, axis=1)\nplt.bar(np.arange(7),label_frac_error)\nplt.xlabel('True Label')\nplt.ylabel('Fraction classified incorrectly')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:13:57.347751Z","iopub.execute_input":"2025-05-12T14:13:57.348055Z","iopub.status.idle":"2025-05-12T14:13:57.584353Z","shell.execute_reply.started":"2025-05-12T14:13:57.348034Z","shell.execute_reply":"2025-05-12T14:13:57.583536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nimport torch\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_roc_for_classes(model, val_loader, num_classes, device):\n    \"\"\"\n    Fonction pour tracer la courbe ROC pour chaque classe.\n    :param model: Modèle PyTorch.\n    :param val_loader: DataLoader pour l'ensemble de validation.\n    :param num_classes: Nombre de classes.\n    :param device: Le périphérique (GPU ou CPU).\n    \"\"\"\n    # Initialisation des labels et des probabilités prédites\n    y_true = []\n    y_scores = []\n\n    model.eval()  # Mettre le modèle en mode évaluation\n\n    with torch.no_grad():  # Pas besoin de calculer les gradients pendant l'évaluation\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Passer les images à travers le modèle\n            outputs = model(images)\n            \n            # Calculer les probabilités via softmax\n            probabilities = torch.softmax(outputs, dim=1)\n            \n            # Ajouter les véritables labels et les probabilités prédites\n            y_true.extend(labels.cpu().numpy())\n            y_scores.extend(probabilities.cpu().numpy())\n\n    # Convertir les labels en format binaire (1 par classe)\n    y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\n\n    # Tracer la courbe ROC pour chaque classe\n    plt.figure(figsize=(10, 8))\n    for i in range(num_classes):\n        # Calculer la courbe ROC\n        fpr, tpr, _ = roc_curve(y_true_bin[:, i], np.array([score[i] for score in y_scores]))\n        roc_auc = auc(fpr, tpr)\n        \n        # Tracer la courbe ROC\n        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n    \n    # Tracer la ligne de non-séparation\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n\n    # Ajouter les labels et légende\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) - Chaque Classe')\n    plt.legend(loc='lower right')\n\n    # Afficher le graphique\n    plt.show()\n\n# Exemple d'utilisation (après l'évaluation du modèle)\nnum_classes = 7  # Nombre de classes dans votre dataset\nplot_roc_for_classes(model, val_loader, num_classes, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:14:01.375196Z","iopub.execute_input":"2025-05-12T14:14:01.375517Z","iopub.status.idle":"2025-05-12T14:14:06.972249Z","shell.execute_reply.started":"2025-05-12T14:14:01.375489Z","shell.execute_reply":"2025-05-12T14:14:06.971267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model39.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:14:11.141943Z","iopub.execute_input":"2025-05-12T14:14:11.142232Z","iopub.status.idle":"2025-05-12T14:14:11.312683Z","shell.execute_reply.started":"2025-05-12T14:14:11.142211Z","shell.execute_reply":"2025-05-12T14:14:11.311998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}